{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab03\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "### Sequence Modeling Exercise\n",
    "+ Large Movie Review Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영화에 대한 느낌을 서술한 글 데이터와 영화 평가(좋게 평가한 것인지, 나쁘게 평가한 것인지) 간의 관계를 순환신경망으로 학습해보기\n",
    "\n",
    "\n",
    "영화 추천 데이터베이스를 이용해 같은 사람이 영화에 대한 느낌을 서술한 글과 영화가 좋은지 나쁜지 별표 등으로 판단한 결과와의 관계를 순환신경망으로 학습 해보겠습니다. \n",
    "\n",
    "학습이 완료된 후 새로운 평가글이 주어졌을 때 판별 결과를 예측하는 것을 목표로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#path 관련 라이브러리\n",
    "import os\n",
    "from os.path import join, isdir\n",
    "\n",
    "# Scientific Math 라이브러리  \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, optimizers, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 데이터 불러오기\n",
    "케라스에서 제공하는 공개 데이터인 IMDB(Internet Movie DataBase)를 사용합니다.\n",
    "\n",
    "IMDB 데이터는 25,000건씩(추천(pos)=1,비추천(neg)=0) 총 50,000개의 영화평과 이진화된 영화 평점 정보를 담고 있습니다.\n",
    "\n",
    "평점 정보는 별점이 많은 경우는 긍정, 그렇지 않은 경우는 부정으로 나눠진 정보입니다.\n",
    "학습을 통해 영화평을 분석해 영화 평점 정보를 예측하도록 학습해 보겠습니다.\n",
    "\n",
    "아래함수를 통해 데이터를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, num_words=None, skip_top=0, seed=113):\n",
    "    #numpy 버전 1.16.3 버전 이상인 경우 allow_pickle을 True로 해주셔야합니다.\n",
    "    with np.load(path, allow_pickle=True) as f:\n",
    "        x_train, labels_train = f['x_train'], f['y_train']\n",
    "        x_test, labels_test = f['x_test'], f['y_test']\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    indices = np.arange(len(x_train))\n",
    "    np.random.shuffle(indices)\n",
    "    x_train = x_train[indices]\n",
    "    labels_train = labels_train[indices]\n",
    "    \n",
    "    indices = np.arange(len(x_test))\n",
    "    np.random.shuffle(indices)\n",
    "    x_test = x_test[indices]\n",
    "    labels_test = labels_test[indices]\n",
    "    \n",
    "    xs = np.concatenate([x_train, x_test])\n",
    "    labels = np.concatenate([labels_train, labels_test])\n",
    "    \n",
    "    if not num_words:\n",
    "        num_words = max([max(x) for x in xs])\n",
    "\n",
    "    xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n",
    "    \n",
    "    idx = len(x_train)\n",
    "    x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
    "    x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = load_data('./data/imdb.npz', num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* train set과 test set에서 10,000개의 sample만 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data partition\n",
    "sampling_indices = np.random.choice(len(train_data), round(len(train_data) * 0.4), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[sampling_indices][:]\n",
    "train_labels = train_labels[sampling_indices][:]\n",
    "test_data = test_data[sampling_indices][:]\n",
    "test_labels = test_labels[sampling_indices][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: (10000,)\n",
      "train_labels shape: (10000,)\n",
      "a train_data sample: [487, 121, 48, 10, 381, 100, 871, 107, 764, 741, 7, 7, 441, 764, 741, 13, 3, 125, 71, 870, 705, 9, 211, 122, 5, 3, 547, 377, 2, 1361, 3858, 18, 31, 1, 55, 9, 13, 117, 42, 3, 7, 7, 42, 251, 5, 615, 6768, 48, 163, 11, 705, 14, 3014, 14, 10, 255, 9, 546, 12, 328, 273, 1, 164, 119, 3, 289, 4, 3658, 22, 80, 1, 203, 4, 48, 59, 897, 25, 74, 3, 518, 4, 1, 4206, 111, 10, 244, 8193, 5, 856, 10, 13, 146, 3, 17, 12, 557, 3, 173, 7, 7, 82, 6226, 10, 101, 23, 1, 4, 1437, 2, 75, 229, 2, 12, 117, 55, 22, 892, 5, 63, 2195, 5, 1, 1437, 3, 706, 43, 4, 155, 50, 37, 3, 690, 454, 18, 195, 181, 49]\n",
      "a train_label sample: 1\n"
     ]
    }
   ],
   "source": [
    "print('train_data shape:', train_data.shape)\n",
    "print('train_labels shape:', train_labels.shape)\n",
    "print('a train_data sample:', train_data[1])\n",
    "print('a train_label sample:', train_labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data의 경우 정수 리스트 형태인 데이터를 확인할 수 있습니다.\n",
    "각 정수는 영화 리뷰에서 한 단어를 나타냅니다. 즉, 영화 리뷰가 정수 리스트로 변환되어 있는 것입니다.<br>\n",
    "(영화 리뷰 문장의 각 단어는 고유한 정수로 인덱싱되어 있습니다.)\n",
    "\n",
    "train_labels의 경우 0 또는 1값을 가지며, 0은 비추천 1은 추천을 의미합니다.\n",
    "<br><br>\n",
    "\n",
    "예시)\n",
    "<img src = \"./Images/reviewsample.png\">\n",
    "\n",
    "<img src = \"./Images/reviewtointlist.png\">\n",
    "(정확한 인덱싱 방법에 대한 설명은 아래 Reference 부분을 참고하시면 됩니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 정수 리스트 단어로 복원해보기\n",
    "\n",
    "단어를 key로 해당 단어의 index를 value로 갖는 딕셔너리를 아래 get_word_index를 통해 불러옵니다. 불러온 딕셔너리를 통해 단어를 복원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_word_index(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fawn', 'tsukino', 'nunnery', 'sonja', 'vani', 'woods', 'spiders', 'hanging', 'woody', 'trawling']\n",
      "you'll\n",
      "know\n",
      "you'll know what i mean after you've seen red eye br br overall red eye was a better than expected thriller it gets off to a slow start and slowly builds but by the time it was over it's a br br it's hard to exactly define what makes this thriller as thrilling as i found it except that simply put the director did a job of pulling you into the action of what would otherwise have been a run of the mill plot i rather tended to forget i was watching a movie that says a lot br br other factors i think are the of victim and bad guy and that over time you begin to really relate to the victim a 8 out of 10 more like a 7 5 but that's pretty good\n"
     ]
    }
   ],
   "source": [
    "# 딕셔너리 불러오기\n",
    "word_to_integer = get_word_index('./data/imdb_word_index.json')\n",
    "\n",
    "# 딕셔너리의 key를 10개 출력합니다.\n",
    "print(list(word_to_integer.keys())[0:10])\n",
    "\n",
    "# 원래 있던 딕셔너리의 key와 value를 바꾼형태의 새로운 딕셔너리를 만듭니다.\n",
    "# key : index\n",
    "# value : 단어\n",
    "reverse_word_to_integer  = dict([(value, key) for (key, value) in word_to_integer.items()])\n",
    "\n",
    "# index에 해당하는 단어를 출력합니다.\n",
    "print(reverse_word_to_integer [487])\n",
    "print(reverse_word_to_integer [121])\n",
    "\n",
    "# we need to subtract 3 from the indices because 0 is 'padding', 1 is 'start of sequence' and 2 is 'unknown'\n",
    "# 딕셔너리의 get(x) 함수는 x라는 key에 대응되는 value를 돌려줍니다\n",
    "# get(x, '디폴트 값') : 딕셔너리 안에 찾으려는 key 값이 없을 경우 디폴트 값을 대신 가져오게 할 수 있습니다.\n",
    "decoded_review = ' '.join([reverse_word_to_integer.get(i, 'UNK') for i in train_data[1]])\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 간단한 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 모델 input size 맞춰주기\n",
    "\n",
    "데이터셋의 문장들은 길이가 다르기 때문에 RNN(LSTM)이 처리하기 적합하도록 길이를 통일하는 작업을 필요로합니다.\n",
    "\n",
    "문장에서 maxlen 이후에 있는 단어들을 케라스 패키지인 sequence에서 제공하는 pad_sequence()함수로 잘라냅니다. 문장 길이가 부족한 경우는 부족한 부분을 0으로 채웁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파라미터 설정\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10000, 80)\n",
      "X_test shape: (10000, 80)\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(train_data, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(test_data, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras로 LSTM 모델 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서\n",
    "# input_shape : input_length 인수로는 순서열의 길이, input_dim 인수로는 벡터의 크기를 입력합니다.\n",
    "# batch size를 제외한 크기를 입력합니다.\n",
    "input_shape = (X_train[0].shape)\n",
    "input_tensor = layers.Input(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "embed_layer = layers.Embedding(max_features, 128)(input_tensor) \n",
    "lstm_layer = layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)(embed_layer)\n",
    "output_tensor = layers.Dense(1, activation='sigmoid')(lstm_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(input_tensor, output_tensor)\n",
    "model.compile(\n",
    "              optimizer='Adam',\n",
    "              # 이진 분류이므로 binary_crossentropy\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 80, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      " - 11s - loss: 0.6791 - acc: 0.5993 - val_loss: 0.5732 - val_acc: 0.6942\n",
      "Epoch 2/15\n",
      " - 10s - loss: 0.5062 - acc: 0.7846 - val_loss: 0.4276 - val_acc: 0.8069\n",
      "Epoch 3/15\n",
      " - 10s - loss: 0.3524 - acc: 0.8565 - val_loss: 0.3896 - val_acc: 0.8257\n",
      "Epoch 4/15\n",
      " - 10s - loss: 0.2473 - acc: 0.9046 - val_loss: 0.4183 - val_acc: 0.8192\n",
      "Epoch 5/15\n",
      " - 10s - loss: 0.1987 - acc: 0.9322 - val_loss: 0.4262 - val_acc: 0.8129\n",
      "Epoch 6/15\n",
      " - 10s - loss: 0.1572 - acc: 0.9475 - val_loss: 0.5343 - val_acc: 0.8089\n",
      "Epoch 7/15\n",
      " - 10s - loss: 0.1297 - acc: 0.9569 - val_loss: 0.5440 - val_acc: 0.8031\n",
      "Epoch 8/15\n",
      " - 10s - loss: 0.1068 - acc: 0.9648 - val_loss: 0.6376 - val_acc: 0.7981\n",
      "Epoch 9/15\n",
      " - 10s - loss: 0.0872 - acc: 0.9735 - val_loss: 0.5879 - val_acc: 0.7982\n",
      "Epoch 10/15\n",
      " - 10s - loss: 0.0783 - acc: 0.9760 - val_loss: 0.7223 - val_acc: 0.7946\n",
      "Epoch 11/15\n",
      " - 10s - loss: 0.0700 - acc: 0.9803 - val_loss: 0.6502 - val_acc: 0.7905\n",
      "Epoch 12/15\n",
      " - 10s - loss: 0.0653 - acc: 0.9789 - val_loss: 0.7521 - val_acc: 0.7926\n",
      "Epoch 13/15\n",
      " - 10s - loss: 0.0548 - acc: 0.9844 - val_loss: 0.7469 - val_acc: 0.7889\n",
      "Epoch 14/15\n",
      " - 10s - loss: 0.0602 - acc: 0.9805 - val_loss: 0.8102 - val_acc: 0.7831\n",
      "Epoch 15/15\n",
      " - 10s - loss: 0.0476 - acc: 0.9857 - val_loss: 0.8054 - val_acc: 0.7820\n",
      "Test score: 0.805386499786377\n",
      "Test accuracy: 0.7820000007629394\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, train_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test, test_labels))\n",
    "score, acc = model.evaluate(X_test, test_labels,\n",
    "                            batch_size=batch_size,\n",
    "                           verbose=2)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비교 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = np.reshape(X_train, (-1, len(X_train[0]), 1))\n",
    "X_test2 = np.reshape(X_test, (-1, len(X_test[0]), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train2 shape: (10000, 80, 1)\n",
      "X_test2 shape: (10000, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_train2 shape:', X_train2.shape)\n",
    "print('X_test2 shape:', X_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model2...\n"
     ]
    }
   ],
   "source": [
    "print('Build model2...')\n",
    "\n",
    "input_shape2 = (X_train2[0].shape)\n",
    "input_tensor2 = layers.Input(input_shape2)\n",
    "\n",
    "# embed_layer = layers.Embedding(max_features, 128)(input_tensor) \n",
    "lstm_layer1 = layers.LSTM(200, dropout=0.2, return_sequences=True, recurrent_dropout=0.2)(input_tensor2)\n",
    "\n",
    "lstm_layer2 = layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)(lstm_layer1)\n",
    "\n",
    "output_tensor2 = layers.Dense(1, activation='sigmoid')(lstm_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Model(input_tensor2, output_tensor2)\n",
    "model2.compile(\n",
    "              optimizer='Adam',\n",
    "              # 이진 분류이므로 binary_crossentropy\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 80, 1)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 80, 200)           161600    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               168448    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 330,177\n",
      "Trainable params: 330,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      " - 24s - loss: 0.6979 - acc: 0.5047 - val_loss: 0.6923 - val_acc: 0.5155\n",
      "Epoch 2/15\n",
      " - 23s - loss: 0.6935 - acc: 0.5142 - val_loss: 0.6922 - val_acc: 0.5188\n",
      "Epoch 3/15\n",
      " - 23s - loss: 0.6925 - acc: 0.5160 - val_loss: 0.6888 - val_acc: 0.5398\n",
      "Epoch 4/15\n",
      " - 23s - loss: 0.6902 - acc: 0.5270 - val_loss: 0.6942 - val_acc: 0.5311\n",
      "Epoch 5/15\n",
      " - 23s - loss: 0.6913 - acc: 0.5285 - val_loss: 0.6864 - val_acc: 0.5464\n",
      "Epoch 6/15\n",
      " - 23s - loss: 0.6904 - acc: 0.5250 - val_loss: 0.6854 - val_acc: 0.5492\n",
      "Epoch 7/15\n",
      " - 23s - loss: 0.6899 - acc: 0.5301 - val_loss: 0.6900 - val_acc: 0.5487\n",
      "Epoch 8/15\n",
      " - 23s - loss: 0.6904 - acc: 0.5308 - val_loss: 0.6890 - val_acc: 0.5288\n",
      "Epoch 9/15\n",
      " - 23s - loss: 0.6883 - acc: 0.5339 - val_loss: 0.6857 - val_acc: 0.5501\n",
      "Epoch 10/15\n",
      " - 23s - loss: 0.6890 - acc: 0.5320 - val_loss: 0.7005 - val_acc: 0.5282\n",
      "Epoch 11/15\n",
      " - 23s - loss: 0.6909 - acc: 0.5316 - val_loss: 0.6880 - val_acc: 0.5359\n",
      "Epoch 12/15\n",
      " - 23s - loss: 0.6894 - acc: 0.5296 - val_loss: 0.6934 - val_acc: 0.5241\n",
      "Epoch 13/15\n",
      " - 23s - loss: 0.6883 - acc: 0.5321 - val_loss: 0.6846 - val_acc: 0.5523\n",
      "Epoch 14/15\n",
      " - 23s - loss: 0.6872 - acc: 0.5376 - val_loss: 0.6855 - val_acc: 0.5499\n",
      "Epoch 15/15\n",
      " - 23s - loss: 0.6871 - acc: 0.5364 - val_loss: 0.6883 - val_acc: 0.5503\n",
      "Test score2: 0.6883054864883423\n",
      "Test accuracy2: 0.5503\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model2.fit(X_train2, train_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test2, test_labels))\n",
    "\n",
    "score2, acc2 = model2.evaluate(X_test2, test_labels,\n",
    "                            batch_size=batch_size,\n",
    "                            verbose = 2)\n",
    "\n",
    "print('Test score2:', score2)\n",
    "print('Test accuracy2:', acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "+ IMDB : https://www.samyzaf.com/ML/imdb/imdb.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sstest",
   "language": "python",
   "name": "sstest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
